# /complete-advanced-setup/deployments/02-main-application.yaml
# Sophisticated FastAPI Application (extracted from portfolio-demo.yaml)
# Features: File upload, processing, storage monitoring, business rules
apiVersion: apps/v1
kind: Deployment
metadata:
  name: contact-api-advanced
  labels:
    app: contact-api
    version: advanced
    component: main-application
spec:
  replicas: 2
  selector:
    matchLabels:
      app: contact-api
      component: main-application
  template:
    metadata:
      labels:
        app: contact-api
        version: advanced
        component: main-application
    spec:
      serviceAccountName: contact-api
      containers:
      - name: fastapi-app
        image: python:3.11-slim
        command: ["/bin/bash"]
        args:
        - -c
        - |
          echo "🚀 Starting Advanced FastAPI Application..."
          
          # Install comprehensive dependencies
          pip install fastapi uvicorn boto3 pydantic[email] python-multipart aiofiles opensearch-py elasticsearch
          
          # Create the sophisticated FastAPI application
          cat > /tmp/advanced_app.py << 'EOF'
          from fastapi import FastAPI, File, UploadFile, HTTPException
          from fastapi.responses import JSONResponse, FileResponse
          from fastapi.middleware.cors import CORSMiddleware
          import os
          import json
          import shutil
          from datetime import datetime
          from pathlib import Path
          import boto3
          from botocore.exceptions import ClientError
          
          app = FastAPI(
              title="Advanced Enterprise File Processing System",
              description="Sophisticated document processing with EFS, S3, OpenSearch, and advanced mounting",
              version="2.0.0"
          )
          
          # CORS middleware
          app.add_middleware(
              CORSMiddleware,
              allow_origins=["*"],
              allow_credentials=True,
              allow_methods=["*"],
              allow_headers=["*"],
          )
          
          # Configuration
          EFS_BASE_PATH = "/mnt/efs"
          UPLOAD_PATH = f"{EFS_BASE_PATH}/uploads"
          PROCESSED_PATH = f"{EFS_BASE_PATH}/processed"
          LOG_PATH = f"{EFS_BASE_PATH}/logs"
          S3_MOUNT_PATH = "/mnt/s3"
          SHARED_DATA_PATH = "/shared-data"
          
          # Initialize S3 client
          try:
              s3_client = boto3.client('s3', region_name=os.environ.get('AWS_REGION', 'ap-southeast-1'))
              print("✅ S3 client initialized successfully")
          except Exception as e:
              print(f"⚠️ S3 client initialization failed: {e}")
              s3_client = None
          
          @app.on_event("startup")
          async def startup_event():
              print("🚀 Advanced Enterprise Application Starting...")
              print(f"📁 EFS Mount Point: {EFS_BASE_PATH}")
              print(f"📁 S3 Mount Point: {S3_MOUNT_PATH}")
              print(f"📁 Shared Data Path: {SHARED_DATA_PATH}")
              print(f"📊 Available Storage: {get_storage_info()}")
              
              # Create directory structure if not exists
              create_directory_structure()
          
          @app.get("/")
          async def root():
              return {
                  "message": "Advanced Enterprise File Processing System",
                  "version": "2.0.0",
                  "features": [
                      "EFS Persistent Storage",
                      "S3 Integration & Mounting",
                      "Document Processing Workflows",
                      "Real-time Monitoring",
                      "OpenSearch Integration",
                      "Multi-container Architecture",
                      "Business Rules Engine",
                      "Automated Backup & Sync"
                  ],
                  "status": "operational",
                  "timestamp": datetime.now().isoformat()
              }
          
          @app.get("/health")
          async def health_check():
              return {
                  "status": "healthy",
                  "timestamp": datetime.now().isoformat(),
                  "storage_status": {
                      "efs_mounted": os.path.exists(EFS_BASE_PATH),
                      "s3_mounted": os.path.exists(S3_MOUNT_PATH),
                      "shared_data": os.path.exists(SHARED_DATA_PATH)
                  },
                  "storage_info": get_storage_info(),
                  "service_health": check_service_health()
              }
          
          @app.get("/storage/status")
          async def storage_status():
              """Advanced storage system monitoring"""
              return {
                  "efs_status": {
                      "mounted": os.path.exists(EFS_BASE_PATH),
                      "upload_dir": os.path.exists(UPLOAD_PATH),
                      "processed_dir": os.path.exists(PROCESSED_PATH),
                      "log_dir": os.path.exists(LOG_PATH)
                  },
                  "s3_status": {
                      "mounted": os.path.exists(S3_MOUNT_PATH),
                      "data_mount": os.path.exists(f"{S3_MOUNT_PATH}/data"),
                      "index_mount": os.path.exists(f"{S3_MOUNT_PATH}/index"),
                      "realtime_mount": os.path.exists(f"{S3_MOUNT_PATH}/realtime")
                  },
                  "file_counts": {
                      "uploads": count_files(UPLOAD_PATH),
                      "processed": count_files(PROCESSED_PATH),
                      "logs": count_files(LOG_PATH)
                  },
                  "storage_info": get_storage_info()
              }
          
          @app.post("/upload")
          async def upload_file(file: UploadFile = File(...)):
              """Advanced document upload with processing pipeline"""
              try:
                  # Validate file
                  if not validate_file(file):
                      raise HTTPException(status_code=400, detail="Invalid file type or size")
                  
                  # Create upload directory structure
                  file_type = get_file_type(file.filename)
                  upload_dir = f"{UPLOAD_PATH}/{file_type}"
                  os.makedirs(upload_dir, exist_ok=True)
                  
                  # Save file to EFS
                  timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                  safe_filename = f"{timestamp}_{file.filename}"
                  file_path = f"{upload_dir}/{safe_filename}"
                  
                  with open(file_path, "wb") as buffer:
                      shutil.copyfileobj(file.file, buffer)
                  
                  # Generate file metadata
                  file_metadata = {
                      "original_name": file.filename,
                      "safe_name": safe_filename,
                      "file_type": file_type,
                      "size": os.path.getsize(file_path),
                      "upload_time": datetime.now().isoformat(),
                      "path": file_path,
                      "status": "uploaded"
                  }
                  
                  # Log the upload
                  log_activity("file_upload", file_metadata)
                  
                  # Trigger processing pipeline
                  processing_result = await trigger_processing_pipeline(file_path, file_metadata)
                  
                  return {
                      "message": "File uploaded successfully",
                      "metadata": file_metadata,
                      "processing": processing_result,
                      "next_steps": ["File queued for processing", "Check /process/status for updates"]
                  }
              except Exception as e:
                  log_activity("upload_error", {"error": str(e), "filename": file.filename})
                  raise HTTPException(status_code=500, detail=str(e))
          
          @app.get("/files")
          async def list_files():
              """Advanced file management system"""
              files = []
              for root, dirs, filenames in os.walk(UPLOAD_PATH):
                  for filename in filenames:
                      file_path = os.path.join(root, filename)
                      try:
                          file_stat = os.stat(file_path)
                          files.append({
                              "name": filename,
                              "path": file_path,
                              "size": file_stat.st_size,
                              "modified": datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                              "type": get_file_type(filename),
                              "status": get_file_status(file_path)
                          })
                      except Exception as e:
                          print(f"Error processing file {filename}: {e}")
              
              return {
                  "files": files, 
                  "count": len(files),
                  "summary": get_files_summary(files)
              }
          
          @app.post("/process/{filename}")
          async def process_file(filename: str):
              """Advanced document processing workflow"""
              try:
                  # Find file in upload directories
                  file_path = find_file(filename)
                  if not file_path:
                      raise HTTPException(status_code=404, detail="File not found")
                  
                  # Load business rules
                  business_rules = load_business_rules()
                  
                  # Create processed directory structure
                  file_type = get_file_type(filename)
                  processed_dir = f"{PROCESSED_PATH}/{file_type}"
                  os.makedirs(processed_dir, exist_ok=True)
                  
                  # Process file based on type and rules
                  processing_result = await process_file_by_type(file_path, file_type, business_rules)
                  
                  # Move to processed directory
                  processed_filename = f"processed_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{filename}"
                  processed_path = f"{processed_dir}/{processed_filename}"
                  shutil.copy2(file_path, processed_path)
                  
                  # Update metadata
                  metadata = {
                      "original_file": file_path,
                      "processed_file": processed_path,
                      "processing_time": datetime.now().isoformat(),
                      "processing_result": processing_result,
                      "business_rules_applied": business_rules.get("document_processing", {})
                  }
                  
                  # Log processing
                  log_activity("file_processing", metadata)
                  
                  # Trigger indexing if enabled
                  if business_rules.get("indexing", {}).get("enabled", False):
                      await trigger_indexing(processed_path, metadata)
                  
                  return {
                      "message": "File processed successfully",
                      "metadata": metadata,
                      "status": "completed"
                  }
              except Exception as e:
                  log_activity("processing_error", {"error": str(e), "filename": filename})
                  raise HTTPException(status_code=500, detail=str(e))
          
          @app.get("/logs")
          async def get_logs():
              """Advanced application monitoring and logging"""
              logs = []
              if os.path.exists(LOG_PATH):
                  for root, dirs, filenames in os.walk(LOG_PATH):
                      for filename in filenames:
                          if filename.endswith('.log'):
                              log_file = os.path.join(root, filename)
                              try:
                                  with open(log_file, 'r') as f:
                                      content = f.read()
                                  logs.append({
                                      "file": filename,
                                      "path": log_file,
                                      "content": content[-1000:],  # Last 1000 chars
                                      "size": len(content),
                                      "modified": datetime.fromtimestamp(os.path.getmtime(log_file)).isoformat()
                                  })
                              except Exception as e:
                                  print(f"Error reading log file {filename}: {e}")
              return {
                  "logs": logs,
                  "count": len(logs),
                  "total_size": sum(log["size"] for log in logs)
              }
          
          @app.get("/business-rules")
          async def get_business_rules():
              """Get current business rules configuration"""
              return load_business_rules()
          
          @app.post("/business-rules")
          async def update_business_rules(rules: dict):
              """Update business rules configuration"""
              try:
                  rules_path = f"{SHARED_DATA_PATH}/config/business-rules.json"
                  os.makedirs(os.path.dirname(rules_path), exist_ok=True)
                  
                  with open(rules_path, 'w') as f:
                      json.dump(rules, f, indent=2)
                  
                  log_activity("business_rules_update", {"rules": rules})
                  
                  return {
                      "message": "Business rules updated successfully",
                      "rules": rules,
                      "timestamp": datetime.now().isoformat()
                  }
              except Exception as e:
                  raise HTTPException(status_code=500, detail=str(e))
          
          # Helper functions
          def create_directory_structure():
              """Create comprehensive directory structure"""
              directories = [
                  f"{EFS_BASE_PATH}/uploads/documents",
                  f"{EFS_BASE_PATH}/uploads/images",
                  f"{EFS_BASE_PATH}/uploads/videos",
                  f"{EFS_BASE_PATH}/processed/documents",
                  f"{EFS_BASE_PATH}/processed/images",
                  f"{EFS_BASE_PATH}/logs/application",
                  f"{EFS_BASE_PATH}/logs/audit",
                  f"{EFS_BASE_PATH}/shared/config",
                  f"{EFS_BASE_PATH}/shared/templates",
                  f"{SHARED_DATA_PATH}/config",
                  f"{SHARED_DATA_PATH}/cache"
              ]
              
              for directory in directories:
                  os.makedirs(directory, exist_ok=True)
          
          def validate_file(file: UploadFile) -> bool:
              """Validate uploaded file"""
              business_rules = load_business_rules()
              max_size = business_rules.get("document_processing", {}).get("max_file_size", "100MB")
              allowed_formats = business_rules.get("document_processing", {}).get("allowed_formats", [])
              
              # Convert max_size to bytes
              max_bytes = parse_size(max_size)
              
              # Check file extension
              if allowed_formats:
                  file_ext = file.filename.split('.')[-1].lower()
                  if file_ext not in allowed_formats:
                      return False
              
              return True
          
          def get_file_type(filename: str) -> str:
              """Determine file type based on extension"""
              ext = filename.split('.')[-1].lower()
              if ext in ['pdf', 'doc', 'docx', 'txt']:
                  return 'documents'
              elif ext in ['jpg', 'jpeg', 'png', 'gif']:
                  return 'images'
              elif ext in ['mp4', 'avi', 'mov']:
                  return 'videos'
              else:
                  return 'other'
          
          def load_business_rules() -> dict:
              """Load business rules from configuration"""
              default_rules = {
                  "document_processing": {
                      "max_file_size": "100MB",
                      "allowed_formats": ["pdf", "docx", "txt", "jpg", "png"],
                      "auto_classification": True,
                      "retention_days": 2555
                  },
                  "workflow": {
                      "approval_required": True,
                      "notification_enabled": True,
                      "backup_enabled": True
                  },
                  "indexing": {
                      "enabled": True,
                      "batch_size": 100,
                      "refresh_interval": "30s"
                  }
              }
              
              try:
                  rules_path = f"{SHARED_DATA_PATH}/config/business-rules.json"
                  if os.path.exists(rules_path):
                      with open(rules_path, 'r') as f:
                          return json.load(f)
              except Exception as e:
                  print(f"Error loading business rules: {e}")
              
              return default_rules
          
          def get_storage_info():
              """Get comprehensive storage information"""
              try:
                  efs_stat = os.statvfs(EFS_BASE_PATH) if os.path.exists(EFS_BASE_PATH) else None
                  
                  if efs_stat:
                      total = efs_stat.f_frsize * efs_stat.f_blocks
                      free = efs_stat.f_frsize * efs_stat.f_available
                      used = total - free
                      
                      return {
                          "efs": {
                              "total_gb": round(total / (1024**3), 2),
                              "used_gb": round(used / (1024**3), 2),
                              "free_gb": round(free / (1024**3), 2),
                              "usage_percent": round((used / total) * 100, 2)
                          },
                          "mounts": {
                              "efs": os.path.exists(EFS_BASE_PATH),
                              "s3": os.path.exists(S3_MOUNT_PATH),
                              "shared": os.path.exists(SHARED_DATA_PATH)
                          }
                      }
              except Exception as e:
                  print(f"Error getting storage info: {e}")
              
              return {"error": "Unable to get storage info"}
          
          def count_files(directory):
              """Count files in directory recursively"""
              try:
                  return sum(len(files) for _, _, files in os.walk(directory))
              except:
                  return 0
          
          def log_activity(activity, data):
              """Log activity to EFS with structured format"""
              try:
                  os.makedirs(f"{LOG_PATH}/application", exist_ok=True)
                  log_file = f"{LOG_PATH}/application/activity.log"
                  
                  log_entry = {
                      "timestamp": datetime.now().isoformat(),
                      "activity": activity,
                      "data": data
                  }
                  
                  with open(log_file, 'a') as f:
                      f.write(f"{json.dumps(log_entry)}\n")
              except Exception as e:
                  print(f"Logging error: {e}")
          
          def parse_size(size_str: str) -> int:
              """Parse size string to bytes"""
              size_str = size_str.upper()
              if size_str.endswith('MB'):
                  return int(size_str[:-2]) * 1024 * 1024
              elif size_str.endswith('GB'):
                  return int(size_str[:-2]) * 1024 * 1024 * 1024
              else:
                  return int(size_str)
          
          def find_file(filename: str) -> str:
              """Find file in upload directories"""
              for root, dirs, files in os.walk(UPLOAD_PATH):
                  if filename in files:
                      return os.path.join(root, filename)
              return None
          
          def get_file_status(file_path: str) -> str:
              """Get file processing status"""
              # Simple status based on location
              if "/processed/" in file_path:
                  return "processed"
              elif "/uploads/" in file_path:
                  return "uploaded"
              else:
                  return "unknown"
          
          def get_files_summary(files: list) -> dict:
              """Get summary statistics for files"""
              total_size = sum(f["size"] for f in files)
              types = {}
              statuses = {}
              
              for f in files:
                  types[f["type"]] = types.get(f["type"], 0) + 1
                  statuses[f["status"]] = statuses.get(f["status"], 0) + 1
              
              return {
                  "total_size_mb": round(total_size / (1024 * 1024), 2),
                  "types": types,
                  "statuses": statuses
              }
          
          def check_service_health() -> dict:
              """Check health of various services"""
              return {
                  "s3_client": s3_client is not None,
                  "efs_writable": check_efs_writable(),
                  "shared_data_writable": check_shared_data_writable()
              }
          
          def check_efs_writable() -> bool:
              """Check if EFS is writable"""
              try:
                  test_file = f"{EFS_BASE_PATH}/health_check.tmp"
                  with open(test_file, 'w') as f:
                      f.write("health_check")
                  os.remove(test_file)
                  return True
              except:
                  return False
          
          def check_shared_data_writable() -> bool:
              """Check if shared data is writable"""
              try:
                  test_file = f"{SHARED_DATA_PATH}/health_check.tmp"
                  with open(test_file, 'w') as f:
                      f.write("health_check")
                  os.remove(test_file)
                  return True
              except:
                  return False
          
          async def trigger_processing_pipeline(file_path: str, metadata: dict) -> dict:
              """Trigger processing pipeline for uploaded file"""
              return {
                  "status": "queued",
                  "pipeline_id": f"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                  "estimated_completion": "5-10 minutes"
              }
          
          async def process_file_by_type(file_path: str, file_type: str, rules: dict) -> dict:
              """Process file based on its type and business rules"""
              processing_steps = []
              
              if file_type == "documents":
                  processing_steps = ["text_extraction", "classification", "indexing"]
              elif file_type == "images":
                  processing_steps = ["thumbnail_generation", "metadata_extraction", "classification"]
              elif file_type == "videos":
                  processing_steps = ["thumbnail_generation", "metadata_extraction", "compression"]
              
              return {
                  "steps_completed": processing_steps,
                  "processing_time_seconds": len(processing_steps) * 2,
                  "rules_applied": rules.get("document_processing", {})
              }
          
          async def trigger_indexing(file_path: str, metadata: dict) -> dict:
              """Trigger OpenSearch indexing"""
              return {
                  "status": "indexed",
                  "index_name": "documents",
                  "document_id": f"doc_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
              }
          
          if __name__ == "__main__":
              import uvicorn
              uvicorn.run(app, host="0.0.0.0", port=8000)
          EOF
          
          # Start the advanced application
          cd /tmp && python advanced_app.py
        ports:
        - containerPort: 8000
        volumeMounts:
        - name: efs-storage
          mountPath: /mnt/efs
        - name: s3-mount
          mountPath: /mnt/s3
        - name: shared-storage
          mountPath: /shared-data
        env:
        - name: AWS_REGION
          value: "ap-southeast-1"
        - name: CONTACTS_TABLE
          value: "realistic-demo-pretamane-contact-submissions"
        - name: VISITORS_TABLE
          value: "realistic-demo-pretamane-website-visitors"
        - name: SES_FROM_EMAIL
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: ses-from-email
        - name: SES_TO_EMAIL
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: ses-to-email
        - name: ALLOWED_ORIGIN
          value: "*"
        - name: EFS_MOUNT_PATH
          value: "/mnt/efs"
        - name: S3_MOUNT_PATH
          value: "/mnt/s3"
        - name: SHARED_DATA_PATH
          value: "/shared-data"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 10
        resources:
          requests:
            memory: "512Mi"
            cpu: "300m"
          limits:
            memory: "1Gi"
            cpu: "600m"
      
      volumes:
      - name: efs-storage
        persistentVolumeClaim:
          claimName: advanced-efs-pvc
      - name: s3-mount
        emptyDir:
          sizeLimit: 1Gi
      - name: shared-storage
        emptyDir:
          sizeLimit: 2Gi
